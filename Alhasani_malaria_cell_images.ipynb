{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bWOxl86i248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Win7\\Desktop\\ML Assignment2\\cell_images\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import sys,os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "path = os.getcwd()\n",
    "path = str(path)+\"\\cell_images\"\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1141,
     "status": "ok",
     "timestamp": 1556808387453,
     "user": {
      "displayName": "batoul alhassany",
      "photoUrl": "",
      "userId": "07438840849110152279"
     },
     "user_tz": -180
    },
    "id": "820nIGaZsqNt",
    "outputId": "53b1802b-47f5-472b-a727-e72a9be24f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1496,
     "status": "ok",
     "timestamp": 1556808387845,
     "user": {
      "displayName": "batoul alhassany",
      "photoUrl": "",
      "userId": "07438840849110152279"
     },
     "user_tz": -180
    },
    "id": "muHUxcbmm3ID",
    "outputId": "4d83f89c-ae6f-411f-8bdf-6a66b98e0dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17638\n",
      "5511\n",
      "4409\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 16\n",
    "# percentage of training set to use as validation\n",
    "test_size = 0.2\n",
    "valid_size = 0.2\n",
    "train_size = 0.8\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "dataset = datasets.ImageFolder(path, transform = transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle = True)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['Parasitized','Uninfected']\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_data = len(dataset)\n",
    "indices = list(range(num_data))\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_size * num_data))\n",
    "train_idx1, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "num_train_data = len(dataset) - split\n",
    "split2 = int(np.floor(valid_size * num_train_data))\n",
    "train_idx, valid_idx = train_idx1[split2:], train_idx1[:split2]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "print(len(train_idx))\n",
    "print(len(test_idx))\n",
    "print(len(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 64, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "for idx in np.arange(1):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    print(images[idx].shape)\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1461,
     "status": "ok",
     "timestamp": 1556808387847,
     "user": {
      "displayName": "batoul alhassany",
      "photoUrl": "",
      "userId": "07438840849110152279"
     },
     "user_tz": -180
    },
    "id": "X_abZZXgrW9O",
    "outputId": "098d97bf-bb6c-4180-eaee-e0a056c691ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#35\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 64x64x3 image tensor)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "\n",
    "        # convolutional layer (sees 32x32x16 tensor)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        \n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.pool1 = nn.MaxPool2d(3, 3)\n",
    "        # linear layer (32 *16*16 -> 512)\n",
    "        self.fc1 = nn.Linear(32 *16*16, 512)\n",
    "        # linear layer (512 -> 256)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "    \n",
    "        # dropout layer (p=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # flatten image input\n",
    "        x = x.view(-1,32 *16*16)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer   \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6D2ECmUm6s7"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2730
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 454374,
     "status": "error",
     "timestamp": 1556808841001,
     "user": {
      "displayName": "batoul alhassany",
      "photoUrl": "",
      "userId": "07438840849110152279"
     },
     "user_tz": -180
    },
    "id": "wBHGXGW-m9mQ",
    "outputId": "e55f25f4-48c0-462f-9193-de131cbabe35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.422972 \tValidation Loss: 0.100240\n",
      "Validation loss decreased (inf --> 0.100240).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.378094 \tValidation Loss: 0.085060\n",
      "Validation loss decreased (0.100240 --> 0.085060).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.270851 \tValidation Loss: 0.041486\n",
      "Validation loss decreased (0.085060 --> 0.041486).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.144192 \tValidation Loss: 0.031112\n",
      "Validation loss decreased (0.041486 --> 0.031112).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.122455 \tValidation Loss: 0.027272\n",
      "Validation loss decreased (0.031112 --> 0.027272).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.110323 \tValidation Loss: 0.029070\n",
      "Epoch: 7 \tTraining Loss: 0.101152 \tValidation Loss: 0.024145\n",
      "Validation loss decreased (0.027272 --> 0.024145).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.094923 \tValidation Loss: 0.023810\n",
      "Validation loss decreased (0.024145 --> 0.023810).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.088949 \tValidation Loss: 0.024037\n",
      "Epoch: 10 \tTraining Loss: 0.084639 \tValidation Loss: 0.023024\n",
      "Validation loss decreased (0.023810 --> 0.023024).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.080216 \tValidation Loss: 0.025666\n",
      "Epoch: 12 \tTraining Loss: 0.072920 \tValidation Loss: 0.022700\n",
      "Validation loss decreased (0.023024 --> 0.022700).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.069675 \tValidation Loss: 0.022692\n",
      "Validation loss decreased (0.022700 --> 0.022692).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.065702 \tValidation Loss: 0.024672\n",
      "Epoch: 15 \tTraining Loss: 0.060802 \tValidation Loss: 0.022399\n",
      "Validation loss decreased (0.022692 --> 0.022399).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.057964 \tValidation Loss: 0.022773\n",
      "Epoch: 17 \tTraining Loss: 0.052880 \tValidation Loss: 0.023310\n",
      "Epoch: 18 \tTraining Loss: 0.049004 \tValidation Loss: 0.022833\n",
      "Epoch: 19 \tTraining Loss: 0.045237 \tValidation Loss: 0.024641\n",
      "Epoch: 20 \tTraining Loss: 0.041512 \tValidation Loss: 0.030628\n",
      "Epoch: 21 \tTraining Loss: 0.037042 \tValidation Loss: 0.026420\n",
      "Epoch: 22 \tTraining Loss: 0.032003 \tValidation Loss: 0.025620\n",
      "Epoch: 23 \tTraining Loss: 0.027800 \tValidation Loss: 0.028516\n",
      "Epoch: 24 \tTraining Loss: 0.024364 \tValidation Loss: 0.027972\n",
      "Epoch: 25 \tTraining Loss: 0.021778 \tValidation Loss: 0.030401\n",
      "Epoch: 26 \tTraining Loss: 0.018455 \tValidation Loss: 0.033348\n",
      "Epoch: 27 \tTraining Loss: 0.015546 \tValidation Loss: 0.033222\n",
      "Epoch: 28 \tTraining Loss: 0.014029 \tValidation Loss: 0.036321\n",
      "Epoch: 29 \tTraining Loss: 0.012695 \tValidation Loss: 0.034873\n",
      "Epoch: 30 \tTraining Loss: 0.009314 \tValidation Loss: 0.037117\n",
      "Epoch: 31 \tTraining Loss: 0.009627 \tValidation Loss: 0.037137\n",
      "Epoch: 32 \tTraining Loss: 0.008521 \tValidation Loss: 0.036522\n",
      "Epoch: 33 \tTraining Loss: 0.005729 \tValidation Loss: 0.039552\n",
      "Epoch: 34 \tTraining Loss: 0.005259 \tValidation Loss: 0.039347\n",
      "Epoch: 35 \tTraining Loss: 0.004129 \tValidation Loss: 0.040218\n",
      "Epoch: 36 \tTraining Loss: 0.003349 \tValidation Loss: 0.045967\n",
      "Epoch: 37 \tTraining Loss: 0.003253 \tValidation Loss: 0.041960\n",
      "Epoch: 38 \tTraining Loss: 0.003043 \tValidation Loss: 0.043889\n",
      "Epoch: 39 \tTraining Loss: 0.006365 \tValidation Loss: 0.042460\n",
      "Epoch: 40 \tTraining Loss: 0.004025 \tValidation Loss: 0.043765\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f0860049b0c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "path = os.getcwd()\n",
    "path = str(path)+\"\\malaria_cnn.pt\"\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "       # torch.save(model.state_dict(), 'model_augmented.pt')\n",
    "        \n",
    "        torch.save(model.state_dict(), path)\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJNV380i3RaL"
   },
   "outputs": [],
   "source": [
    "#Load the Model with the Lowest Validation Loss\n",
    "model.load_state_dict(torch.load('malaria_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1322,
     "status": "ok",
     "timestamp": 1556808881704,
     "user": {
      "displayName": "batoul alhassany",
      "photoUrl": "",
      "userId": "07438840849110152279"
     },
     "user_tz": -180
    },
    "id": "3iPqrZ44tjyW",
    "outputId": "28b2ba2d-e995-4ffa-b69b-f97cae752447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.028535\n",
      "\n",
      "Test Accuracy of Parasitized: 94% (2611/2756)\n",
      "Test Accuracy of Uninfected: 96% (2656/2755)\n",
      "\n",
      "Test Accuracy (Overall): 95% (5267/5511)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "   # for i in range(batch_size):\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        if(pred[i] == label):\n",
    "           class_correct[label] += 1\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(2):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#loading the pretrained model \n",
    "model = models.vgg16(pretrained = True)\n",
    "\n",
    "#Freeze the parameters for the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#view all the layer on VGG16 \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(25088,4096)\n",
    "        self.hidden2 = nn.Linear(4096, 4096)\n",
    "        self.output = nn.Linear(4096, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dropout(F.relu(self.hidden1(x)))\n",
    "        x = self.dropout(F.relu(self.hidden2(x)))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Classifier(\n",
      "    (hidden1): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (hidden2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (output): Linear(in_features=4096, out_features=2, bias=True)\n",
      "    (dropout): Dropout(p=0.5)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#replace the model's default \n",
    "model.classifier = Classifier()\n",
    "\n",
    "print(model)\n",
    "\n",
    "#\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "#define the lost function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr = 0.001, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.202403 \tValidation Loss: 0.044226\n",
      "Validation loss decreased (inf --> 0.044226).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.157769 \tValidation Loss: 0.033224\n",
      "Validation loss decreased (0.044226 --> 0.033224).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.144502 \tValidation Loss: 0.032636\n",
      "Validation loss decreased (0.033224 --> 0.032636).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.140496 \tValidation Loss: 0.032179\n",
      "Validation loss decreased (0.032636 --> 0.032179).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.133009 \tValidation Loss: 0.029992\n",
      "Validation loss decreased (0.032179 --> 0.029992).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.128775 \tValidation Loss: 0.032305\n",
      "Epoch: 7 \tTraining Loss: 0.125876 \tValidation Loss: 0.031427\n",
      "Epoch: 8 \tTraining Loss: 0.122524 \tValidation Loss: 0.030548\n",
      "Epoch: 9 \tTraining Loss: 0.119152 \tValidation Loss: 0.029482\n",
      "Validation loss decreased (0.029992 --> 0.029482).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.118722 \tValidation Loss: 0.029014\n",
      "Validation loss decreased (0.029482 --> 0.029014).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.116552 \tValidation Loss: 0.028628\n",
      "Validation loss decreased (0.029014 --> 0.028628).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.113172 \tValidation Loss: 0.028163\n",
      "Validation loss decreased (0.028628 --> 0.028163).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.111522 \tValidation Loss: 0.028319\n",
      "Epoch: 14 \tTraining Loss: 0.110991 \tValidation Loss: 0.029344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9010f324f573>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;31m# update training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "path = str(path)+\"\\malaria_transfer_learning.pt\"\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "       # torch.save(model.state_dict(), 'model_augmented.pt')\n",
    "        torch.save(model.state_dict(),path)\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Model with the Lowest Validation Loss\n",
    "\n",
    "model.load_state_dict(torch.load('malaria_transfer_learning.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "   # for i in range(batch_size):\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        if(pred[i] == label):\n",
    "           class_correct[label] += 1\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(2):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of CNN (50rps).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
